///|
/// Lexer error type
pub suberror LexError {
  UnexpectedChar(Int, Char)
  UnterminatedString(Int)
  InvalidNumber(Int, String)
  InvalidEscape(Int, Char)
} derive(Show, Eq)

///|
/// Lexer state
priv struct Lexer {
  input : String
  mut pos : Int
  mut tokens : Array[Token]
}

///|
/// Create a new lexer
fn Lexer::new(input : String) -> Lexer {
  { input, pos: 0, tokens: [] }
}

///|
/// Get current character
fn Lexer::current(self : Lexer) -> Char? {
  if self.pos < self.input.length() {
    self.input.get_char(self.pos)
  } else {
    None
  }
}

///|
/// Peek at next character
fn Lexer::peek(self : Lexer) -> Char? {
  if self.pos + 1 < self.input.length() {
    self.input.get_char(self.pos + 1)
  } else {
    None
  }
}

///|
/// Advance position
fn Lexer::advance(self : Lexer) -> Unit {
  self.pos = self.pos + 1
}

///|
/// Add a token
fn Lexer::add_token(self : Lexer, token : Token) -> Unit {
  self.tokens = self.tokens + [token]
}

///|
/// Skip whitespace
fn Lexer::skip_whitespace(self : Lexer) -> Unit {
  while true {
    match self.current() {
      Some(' ') | Some('\n') | Some('\r') | Some('\t') => self.advance()
      _ => break
    }
  }
}

///|
/// Skip comment (# to end of line)
fn Lexer::skip_comment(self : Lexer) -> Unit {
  while true {
    match self.current() {
      Some('\n') | None => break
      _ => self.advance()
    }
  }
}

///|
/// Check if character is digit
fn is_digit(c : Char) -> Bool {
  c >= '0' && c <= '9'
}

///|
/// Check if character is letter or underscore
fn is_alpha(c : Char) -> Bool {
  (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || c == '_'
}

///|
/// Check if character is alphanumeric or underscore
fn is_alnum(c : Char) -> Bool {
  is_alpha(c) || is_digit(c)
}

///|
/// Lex a number
fn Lexer::lex_number(self : Lexer) -> Unit raise LexError {
  let start = self.pos
  let mut has_dot = false
  if self.current() == Some('-') {
    self.advance()
  }
  while true {
    match self.current() {
      Some(c) if is_digit(c) => self.advance()
      Some('.') if not(has_dot) && self.peek().map(is_digit).unwrap_or(false) => {
        has_dot = true
        self.advance()
      }
      _ => break
    }
  }
  let text = self.input[start:self.pos].to_string() catch {
    _ => raise InvalidNumber(start, "invalid slice")
  }
  let n = @strconv.parse_double(text) catch {
    _ => raise InvalidNumber(start, text)
  }
  self.add_token(TNumber(n))
}

///|
/// Lex a string (with interpolation support)
fn Lexer::lex_string(self : Lexer) -> Unit raise LexError {
  let start = self.pos
  self.advance()
  let parts : Array[(String, String?)] = []
  let buf = @buffer.new()
  while true {
    match self.current() {
      None => raise UnterminatedString(start)
      Some('"') => {
        self.advance()
        break
      }
      Some('\\') => {
        self.advance()
        match self.current() {
          None => raise UnterminatedString(start)
          Some('(') => {
            // String interpolation: \\(expr)
            self.advance()
            parts.push((buf.to_string(), None))
            buf.reset()
            // Find the matching closing paren
            let expr_start = self.pos
            let mut paren_depth = 1
            while paren_depth > 0 {
              match self.current() {
                None => raise UnterminatedString(start)
                Some('(') => {
                  paren_depth = paren_depth + 1
                  self.advance()
                }
                Some(')') => {
                  paren_depth = paren_depth - 1
                  if paren_depth == 0 {
                    let expr_text = try! self.input[expr_start:self.pos].to_string()
                    parts.push(("", Some(expr_text)))
                    self.advance()
                  } else {
                    self.advance()
                  }
                }
                Some('"') => {
                  // Skip nested strings
                  self.advance()
                  while true {
                    match self.current() {
                      Some('"') => {
                        self.advance()
                        break
                      }
                      Some('\\') => {
                        self.advance()
                        self.advance()
                      }
                      None => raise UnterminatedString(start)
                      _ => self.advance()
                    }
                  }
                }
                _ => self.advance()
              }
            }
          }
          Some('n') => {
            buf.write_char('\n')
            self.advance()
          }
          Some('r') => {
            buf.write_char('\r')
            self.advance()
          }
          Some('t') => {
            buf.write_char('\t')
            self.advance()
          }
          Some('\\') => {
            buf.write_char('\\')
            self.advance()
          }
          Some('"') => {
            buf.write_char('"')
            self.advance()
          }
          Some('/') => {
            buf.write_char('/')
            self.advance()
          }
          Some('b') => {
            buf.write_char('\b')
            self.advance()
          }
          Some(c) => raise InvalidEscape(self.pos, c)
        }
      }
      Some(c) => {
        buf.write_char(c)
        self.advance()
      }
    }
  }
  parts.push((buf.to_string(), None))
  if parts.length() == 1 && parts[0].1 == None {
    self.add_token(TString(parts[0].0))
  } else {
    self.add_token(TStringInterpolation(parts))
  }
}

///|
/// Lex an identifier or keyword
fn Lexer::lex_identifier(self : Lexer) -> Unit {
  let start = self.pos
  while self.current().map(is_alnum).unwrap_or(false) {
    self.advance()
  }
  let text = try! self.input[start:self.pos].to_string()
  match text {
    "true" => self.add_token(TTrue)
    "false" => self.add_token(TFalse)
    "null" => self.add_token(TNull)
    "and" => self.add_token(TAnd)
    "or" => self.add_token(TOr)
    "not" => self.add_token(TNot)
    "if" => self.add_token(TIf)
    "then" => self.add_token(TThen)
    "else" => self.add_token(TElse)
    "elif" => self.add_token(TElif)
    "end" => self.add_token(TEnd)
    "as" => self.add_token(TAs)
    "reduce" => self.add_token(TReduce)
    "foreach" => self.add_token(TForeach)
    "try" => self.add_token(TTry)
    "catch" => self.add_token(TCatch)
    "def" => self.add_token(TDef)
    _ => self.add_token(TIdentifier(text))
  }
}

///|
/// Lex a variable ($name)
fn Lexer::lex_variable(self : Lexer) -> Unit {
  self.advance() // skip $
  let start = self.pos
  while self.current().map(is_alnum).unwrap_or(false) {
    self.advance()
  }
  let name = try! self.input[start:self.pos].to_string()
  self.add_token(TVariable(name))
}

///|
/// Lex a format (@name)
fn Lexer::lex_format(self : Lexer) -> Unit {
  self.advance() // skip @
  let start = self.pos
  while self.current().map(is_alnum).unwrap_or(false) {
    self.advance()
  }
  let name = try! self.input[start:self.pos].to_string()
  self.add_token(TFormat(name))
}

///|
/// Main lexing function
pub fn lex(input : String) -> Array[Token] raise LexError {
  let lexer = Lexer::new(input)
  while true {
    lexer.skip_whitespace()
    match lexer.current() {
      None => break
      Some('#') => lexer.skip_comment()
      Some('"') => lexer.lex_string()
      Some('$') => lexer.lex_variable()
      Some('@') => lexer.lex_format()
      Some(c) if is_digit(c) ||
        (c == '-' && lexer.peek().map(is_digit).unwrap_or(false)) =>
        lexer.lex_number()
      Some(c) if is_alpha(c) => lexer.lex_identifier()
      Some(c) =>
        match c {
          '.' => {
            lexer.advance()
            if lexer.current() == Some('.') {
              lexer.advance()
              lexer.add_token(TDotDot)
            } else {
              lexer.add_token(TDot)
            }
          }
          '|' => {
            lexer.advance()
            if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TUpdate)
            } else {
              lexer.add_token(TPipe)
            }
          }
          '/' => {
            lexer.advance()
            if lexer.current() == Some('/') {
              lexer.advance()
              if lexer.current() == Some('=') {
                lexer.advance()
                lexer.add_token(TAltAssign)
              } else {
                lexer.add_token(TAlternative)
              }
            } else if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TDivAssign)
            } else {
              lexer.add_token(TSlash)
            }
          }
          '=' => {
            lexer.advance()
            if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TEq)
            } else {
              lexer.add_token(TAssign)
            }
          }
          '!' => {
            lexer.advance()
            if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TNeq)
            } else {
              raise UnexpectedChar(lexer.pos, c)
            }
          }
          '<' => {
            lexer.advance()
            if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TLe)
            } else {
              lexer.add_token(TLt)
            }
          }
          '>' => {
            lexer.advance()
            if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TGe)
            } else {
              lexer.add_token(TGt)
            }
          }
          ',' => {
            lexer.advance()
            lexer.add_token(TComma)
          }
          ':' => {
            lexer.advance()
            lexer.add_token(TColon)
          }
          ';' => {
            lexer.advance()
            lexer.add_token(TSemicolon)
          }
          '?' => {
            lexer.advance()
            lexer.add_token(TQuestion)
          }
          '(' => {
            lexer.advance()
            lexer.add_token(TLParen)
          }
          ')' => {
            lexer.advance()
            lexer.add_token(TRParen)
          }
          '[' => {
            lexer.advance()
            lexer.add_token(TLBracket)
          }
          ']' => {
            lexer.advance()
            lexer.add_token(TRBracket)
          }
          '{' => {
            lexer.advance()
            lexer.add_token(TLBrace)
          }
          '}' => {
            lexer.advance()
            lexer.add_token(TRBrace)
          }
          '+' => {
            lexer.advance()
            if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TAddAssign)
            } else {
              lexer.add_token(TPlus)
            }
          }
          '-' => {
            lexer.advance()
            if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TSubAssign)
            } else {
              lexer.add_token(TMinus)
            }
          }
          '*' => {
            lexer.advance()
            if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TMulAssign)
            } else {
              lexer.add_token(TStar)
            }
          }
          '%' => {
            lexer.advance()
            if lexer.current() == Some('=') {
              lexer.advance()
              lexer.add_token(TModAssign)
            } else {
              lexer.add_token(TPercent)
            }
          }
          _ => raise UnexpectedChar(lexer.pos, c)
        }
    }
  }
  lexer.add_token(TEof)
  lexer.tokens
}
